{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Load the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Test model and parameters for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'Linear Regression': {},\n",
    "    'Random Forest': {'model__n_estimators': [100, 200], 'model__max_depth': [None, 10, 20]},\n",
    "    'Gradient Boosting': {'model__n_estimators': [100, 200], 'model__learning_rate': [0.01, 0.1]},\n",
    "    'XGBoost': {'model__n_estimators': [100, 200], 'model__learning_rate': [0.01, 0.1], 'model__max_depth': [3, 5]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  Methods used for various tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_v1(df):\n",
    "    # Set up the plot\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "    # Define width of bars and positions\n",
    "    bar_width = 0.12  # Increased for better spacing\n",
    "    metric_columns = df.columns[1:]  # Exclude the 'Model' column\n",
    "    num_metrics = len(metric_columns)\n",
    "    num_models = len(df)\n",
    "\n",
    "    # Create bars for each model\n",
    "    for i, (index, row) in enumerate(df.iterrows()):\n",
    "        x = np.arange(num_metrics) + i * bar_width\n",
    "        bars = ax.bar(x, row[1:], width=bar_width, label=row['Model'])\n",
    "\n",
    "        # Add value labels on the bars\n",
    "        for j, v in enumerate(row[1:]):\n",
    "            if v > 0:  # Only display labels for non-zero bars\n",
    "                ax.text(x[j], v, f'{v:.4f}', ha='center', va='bottom', rotation=30, fontsize=10, color='black')\n",
    "\n",
    "    # Customize the plot\n",
    "    ax.set_xlabel('Metrics', fontweight='bold', fontsize=18)\n",
    "    ax.set_ylabel('Scores', fontweight='bold', fontsize=18)\n",
    "    ax.set_title('Comparison of Metrics Across Models', fontweight='bold', fontsize=26, pad=20)\n",
    "    ax.set_xticks(np.arange(num_metrics) + bar_width * (num_models - 1) / 2)\n",
    "    ax.set_xticklabels(metric_columns, rotation=45, ha='right', fontsize=14)\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend(title='Models', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=14, title_fontsize=16)\n",
    "\n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_v2(df):\n",
    "    # Set up the plot\n",
    "    fig, ax = plt.subplots(figsize=(20, 12))\n",
    "\n",
    "    # Define width of bars and positions\n",
    "    bar_width = 0.15  # Increased for better spacing\n",
    "    metric_columns = df.columns[1:]  # Exclude the 'Model' column\n",
    "    num_metrics = len(metric_columns)\n",
    "    num_models = len(df)\n",
    "\n",
    "    # Create bars for each metric\n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        x = np.arange(num_models) + i * bar_width\n",
    "        bars = ax.bar(x, df[metric], width=bar_width, label=metric)\n",
    "        \n",
    "        # Add value labels on the bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:  # Only display labels for non-zero bars\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{height:.3f}', ha='center', va='bottom', rotation=30, fontsize=10, color='black')\n",
    "\n",
    "    # Customize the plot\n",
    "    ax.set_xlabel('Models', fontweight='bold', fontsize=18)\n",
    "    ax.set_ylabel('Scores', fontweight='bold', fontsize=18)\n",
    "    ax.set_title('Comparison of Metrics Across Models', fontweight='bold', fontsize=26, pad=20)\n",
    "    ax.set_xticks(np.arange(num_models) + bar_width * (num_metrics - 1) / 2)\n",
    "    ax.set_xticklabels(df['Model'], rotation=45, ha='right', fontsize=14)\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend(title='Metrics', bbox_to_anchor=(1.01, 1), loc='upper left', fontsize=14, title_fontsize=16)\n",
    "\n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_v3(df):\n",
    "    metrics = df.columns[1:]  # Exclude 'Model' column\n",
    "    num_metrics = len(metrics)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=(num_metrics + 2) // 3, ncols=3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        axes[i].bar(df['Model'], df[metric], color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "        axes[i].set_title(metric, fontsize=14, fontweight='bold')\n",
    "        axes[i].set_ylabel(metric, fontsize=12)\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Add value labels on the bars\n",
    "        for j, value in enumerate(df[metric]):\n",
    "            axes[i].text(j, value, f'{value:.3f}', ha='center', va='bottom', fontsize=10, rotation=45)\n",
    "\n",
    "    # Hide any empty subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, threshold=5.0):\n",
    "    correct_predictions = np.abs(y_true - y_pred) <= threshold\n",
    "    accuracy = np.sum(correct_predictions) / len(y_true)\n",
    "    return {\n",
    "        'R²': r2_score(y_true, y_pred),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(root_mean_squared_error(y_true, y_pred)),\n",
    "        'Accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_training(X, y, test_size=0.2, random_state=42):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    trained_models = {}\n",
    "\n",
    "    for name, model in tqdm(models.items(), desc=\"Training Models\"):\n",
    "        # Define the pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "            ('model', model)\n",
    "        ])        \n",
    "    \n",
    "        # Fit the pipeline\n",
    "        pipeline.fit(X_train, y_train)\n",
    "    \n",
    "        # Make predictions\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "            \n",
    "        metrics = calculate_metrics(y_test, y_pred)\n",
    "        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')\n",
    "        metrics['Cross-Val R²'] = np.mean(cv_scores)\n",
    "        \n",
    "        # Add model name to metrics\n",
    "        metrics['Model'] = name\n",
    "        \n",
    "        # Append results\n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Store the trained pipeline\n",
    "        trained_models[name] = pipeline\n",
    "                \n",
    "    # Convert the results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Reorder columns to have 'Model' as the first column\n",
    "    cols = ['Model'] + [col for col in results_df.columns if col != 'Model']\n",
    "    results_df = results_df[cols]\n",
    "\n",
    "    # Return the DataFrame and trained models\n",
    "    return results_df, trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  Read the dataset that resulted from Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dataset = pd.read_csv('Experiment_3_filtered_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = experiment_dataset.drop(['SHAP Match Percentage'], axis=1)\n",
    "y = experiment_dataset['SHAP Match Percentage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP - 1: Test the models with Standard dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_exp_1,trained_model_1 = execute_training(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_exp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph_v1(result_exp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph_v2(result_exp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph_v3(result_exp_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP - 2: Test the models with Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Generate some new feature based on data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "X['RAE_sMAPE_interaction'] = X['RAE'] * X['sMAPE']\n",
    "X['log_MedAE'] = np.log1p(X['MedAE'])\n",
    "X['log_Max_Error'] = np.log1p(X['Max Error'])\n",
    "X['Sample_Size_R2'] = X['Sample Size'] * X['R²']\n",
    "X['negative_r2'] = X['R²'] < 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "\n",
    "k = 5  # THIS CAN BE CHANGED\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression, k=k)\n",
    "selector.fit(X, y)\n",
    "cols = selector.get_support(indices=True)\n",
    "X_selected = X.iloc[:, cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  Final Features Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine selected features with engineered features\n",
    "engineered_features = ['RAE_sMAPE_interaction', 'log_MedAE', 'log_Max_Error', 'Sample_Size_R2', 'negative_r2']\n",
    "\n",
    "features_with_feature_engineering = list(X_selected.columns) + engineered_features\n",
    "\n",
    "# Create final feature set\n",
    "X_with_feature_engineering = X[features_with_feature_engineering]\n",
    "print(\"Features with feature enginering:\", features_with_feature_engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_exp_2,trained_model_2 = execute_training(X_with_feature_engineering, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_exp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph_v1(result_exp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph_v2(result_exp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph_v3(result_exp_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 3: Test the models with GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_test, y_pred, model):\n",
    "    # Calculate the new metrics\n",
    "    rmse = np.sqrt(root_mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    correct_predictions = np.abs(y_test - y_pred) <= 5.0\n",
    "\n",
    "    accuracy = np.sum(correct_predictions) / len(y_test)\n",
    "\n",
    "\n",
    "    print(f\"Optimized {model}: RMSE = {rmse:.2f}, R² = {r2:.2f} Accuracy =  {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 3\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mX\u001b[49m, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define models to test\u001b[39;00m\n\u001b[1;32m      6\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinear Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: LinearRegression(),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient Boosting\u001b[39m\u001b[38;5;124m'\u001b[39m: GradientBoostingRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m'\u001b[39m: XGBRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     11\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'Linear Regression': {},\n",
    "    'Random Forest': {'model__n_estimators': [100, 200], 'model__max_depth': [None, 10, 20]},\n",
    "    'Gradient Boosting': {'model__n_estimators': [100, 200], 'model__learning_rate': [0.01, 0.1]},\n",
    "    'XGBoost': {'model__n_estimators': [100, 200], 'model__learning_rate': [0.01, 0.1], 'model__max_depth': [3, 5]}\n",
    "}\n",
    "\n",
    "# Test models with cross-validation and hyperparameter tuning\n",
    "results = {}\n",
    "for name, model in tqdm(models.items(), desc=\"Training Models\"):\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grids[name], cv=5, scoring='r2', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    results[name] = calculate_metrics(y_test, y_pred)\n",
    "    results[name]['Best Parameters'] = grid_search.best_params_\n",
    "    results[name]['Cross-Val R²'] = np.mean(cross_val_score(grid_search.best_estimator_, X_train, y_train, cv=5, scoring='r2'))\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "# Display the DataFrame nicely formatted\n",
    "results_df_formatted = results_df.style.set_caption(\"Model Testing Results\").format(\"{:.4f}\", subset=[\"R²\", \"MAE\", \"RMSE\", \"Precision\", \"Accuracy\", \"Cross-Val R²\"])\n",
    "\n",
    "# Return the DataFrame as the output\n",
    "results_df_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best model based on Cross-Val R²\n",
    "best_model = max(results, key=lambda x: results[x]['Cross-Val R²'])\n",
    "print(f\"\\nBest model based on Cross-Val R²: {best_model}\")\n",
    "print(f\"Best Cross-Val R²: {results[best_model]['Cross-Val R²']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 4: Test the two best models XGBoost and Random Forest with RandomizedSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best parameters: {'subsample': 1.0, 'reg_lambda': 1.0, 'reg_alpha': 1.0, 'n_estimators': 500, 'max_depth': 10, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 0.6}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.3, 0.5],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],  # L1 regularization\n",
    "    'reg_lambda': [0.5, 1.0, 1.5]     # L2 regularization\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = XGBRegressor(random_state=42)\n",
    "\n",
    "# Randomized search with 5-fold cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, \n",
    "                                   param_distributions=param_grid, \n",
    "                                   n_iter=50, \n",
    "                                   cv=5, \n",
    "                                   verbose=1, \n",
    "                                   random_state=42, \n",
    "                                   n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model_xgb = random_search.best_estimator_\n",
    "y_pred_xgb = best_model_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized XGBoost: RMSE = 2.25, R² = 0.78 Accuracy =  0.87\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_test, y_pred_xgb, 'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/OPIT/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/OPIT/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "45 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/OPIT/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/anaconda3/envs/OPIT/lib/python3.11/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/homebrew/anaconda3/envs/OPIT/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/homebrew/anaconda3/envs/OPIT/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/OPIT/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/anaconda3/envs/OPIT/lib/python3.11/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/homebrew/anaconda3/envs/OPIT/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/homebrew/anaconda3/envs/OPIT/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/homebrew/anaconda3/envs/OPIT/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.52482263 0.54557616 0.5129128  0.71436004\n",
      " 0.52416385 0.52472284 0.54778294        nan 0.49396202 0.52453664\n",
      " 0.4833481  0.681362          nan        nan 0.54762223 0.51322823\n",
      " 0.68646651 0.5127337  0.5154996  0.49399621 0.70851601 0.6833185\n",
      " 0.48163824 0.48163485 0.55149895 0.52363098 0.52341276 0.49396048\n",
      "        nan 0.55596271        nan 0.55176838 0.55749128        nan\n",
      " 0.705039   0.62822351 0.49396202 0.52441425 0.49392851 0.51283392\n",
      " 0.54465605 0.51542027 0.52434165 0.52363098 0.51295977 0.550849\n",
      " 0.51511664        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Randomized search with 5-fold cross-validation\n",
    "random_search_rf = RandomizedSearchCV(estimator=rf_model,\n",
    "                                      param_distributions=param_grid,\n",
    "                                      n_iter=50,\n",
    "                                      cv=5,\n",
    "                                      verbose=1,\n",
    "                                      random_state=42,\n",
    "                                      n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_rf = random_search_rf.best_params_\n",
    "print(f\"Best parameters for Random Forest: {best_params_rf}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_rf_model = random_search_rf.best_estimator_\n",
    "y_pred_rf = best_rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Random Forest: RMSE = 2.26, R² = 0.77 Accuracy =  0.85\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_test, y_pred_rf, 'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 4: Test the two best models XGBoost and Random Forest with RandomizedSearchCV in ensembled mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You already have these two trained models:\n",
    "# - best_rf_model from the RandomizedSearchCV for RandomForest\n",
    "# - best_model_xgb from the RandomizedSearchCV for XGBoost\n",
    "\n",
    "# Define the base models for stacking\n",
    "estimators = [\n",
    "    ('random_forest', best_rf_model),\n",
    "    ('xgboost', best_model_xgb)\n",
    "]\n",
    "\n",
    "# Define the meta-model (here, Ridge Regression)\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "\n",
    "# Create the Stacking Regressor\n",
    "stacking_model = StackingRegressor(estimators=estimators, final_estimator=meta_model, cv=5)\n",
    "\n",
    "# Fit the stacking ensemble model\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_stacking_1 = stacking_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Stacking Ensemble - using Ridge: RMSE = 2.23, R² = 0.78 Accuracy =  0.87\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_test, y_pred_stacking_1,'Stacking Ensemble - using Ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base models for stacking\n",
    "estimators = [\n",
    "    ('random_forest', best_rf_model),\n",
    "    ('xgboost', best_model_xgb)\n",
    "]\n",
    "\n",
    "# Define the meta-model (here, GradientBoostingRegressor)\n",
    "meta_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "\n",
    "# Create the Stacking Regressor\n",
    "stacking_model = StackingRegressor(estimators=estimators, final_estimator=meta_model, cv=10) # Previously tried 5\n",
    "\n",
    "# Fit the stacking ensemble model\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_stacking = stacking_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Stacking Ensemble - using Gradient Boosting: RMSE = 2.21, R² = 0.79 Accuracy =  0.89\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_test, y_pred_stacking, 'Stacking Ensemble - using Gradient Boosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "stacking_models = [\n",
    "    StackingRegressor(estimators=estimators, final_estimator=Ridge(), cv=5),\n",
    "    StackingRegressor(estimators=estimators, final_estimator=GradientBoostingRegressor(), cv=5),\n",
    "    StackingRegressor(estimators=estimators, final_estimator=ElasticNet(), cv=5)\n",
    "]\n",
    "\n",
    "y_pred_ensemble_multiple_models = np.mean([model.fit(X_train, y_train).predict(X_test) for model in stacking_models], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Stacking Ensemble - using multiple models: RMSE = 2.22, R² = 0.79 Accuracy =  0.88\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_test, y_pred_ensemble_multiple_models, 'Stacking Ensemble - using multiple models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bagged_stacking = BaggingRegressor(estimator=stacking_model, n_estimators=10, random_state=42)\n",
    "bagged_stacking.fit(X_train, y_train)\n",
    "y_pred_bagged = bagged_stacking.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, y_pred_bagged, 'Stacking Ensemble - using Bagging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Assuming 'stacking_model' is your fitted model\n",
    "\n",
    "# Create a directory to save the model (if it doesn't exist)\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Save the model using pickle\n",
    "with open('saved_models/stacking_model.pkl', 'wb') as file:\n",
    "    pickle.dump(stacking_model, file)\n",
    "\n",
    "# Alternatively, you can use joblib, which is more efficient for large numpy arrays\n",
    "joblib.dump(stacking_model, 'saved_models/stacking_model.joblib')\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OPIT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
